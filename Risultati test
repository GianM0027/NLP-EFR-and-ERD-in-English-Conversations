
SEED DELLE ESECUZIONI --> ancora 42, sempre


##############################################################################################################
##############################################################################################################
##############################################################################################################
####################    CLS all'inizio di ogni frase invece che a inizio dialogo    ##########################

NO SPEAKER (BertFreezed):
emotions F1 a inizio training: 0.31781454253442
triggers F1 a inizio training: 0.546869906891933

emotions F1 dopo 6 epoche: 0.397655514971359
triggers F1 dopo 6 epoche: 0.452217648883609


CON SPEAKERS (BertFreezed):
emotions F1 a inizio training: 0.256253782780491
triggers F1 a inizio training: 0.571547139423611

emotions F1 dopo 6 epoche: 0.365639745118752
triggers F1 dopo 6 epoche: 0.613461381390267


CON SPEAKER E LINEAR LAYER AGGIUNTIVO da 512 (BertFreezed):
emotions F1 a inizio training: 0.358420089631834
triggers F1 a inizio training: 0.489194671607462

emotions F1 dopo 6 epoche: 0.398655886368746
triggers F1 dopo 6 epoche: 0.543350409214008



##############################################################################################################
##############################################################################################################
##############################################################################################################
####################    CLS all'inizio di ogni dialogo (concettualmente più giusto)    #######################

NO SPEAKER (emotion: 0.65/ triggers: 0.35)(BertFreezed):
emotions F1 a inizio training: 0.310845833066838
triggers F1 a inizio training: 0.555928575087981

emotions F1 dopo 6 epoche: 0.411869032755343
triggers F1 dopo 6 epoche: 0.581484665854188 (leggero overfitting nell'ultima epoca)


NO SPEAKER (emotion: 1/ triggers: 0.375)(BertFreezed):
emotions F1 a inizio training: 0.30896897280844
triggers F1 a inizio training: 0.569543674218896

emotions F1 dopo 6 epoche: 0.445148108733262
triggers F1 dopo 6 epoche: 0.615729224766461

emotions F1 dopo 15 epoche: 0.525177927565415
triggers F1 dopo 15 epoche: 0.630012371118332


NO SPEAKER (emotion: 1/ triggers: 0.375)(BertFull):
emotions F1 a inizio training: 0.434187772778897
triggers F1 a inizio training: 0.611406103726913

emotions F1 dopo 6 epoche: 0.572900666251173
triggers F1 dopo 6 epoche: 0.651206347835263

emotions F1 dopo 10 epoche: 0.588201131260076
triggers F1 dopo 10 epoche: 0.644205821962442

emotions F1 dopo 15 epoche: 0.591197729576735
triggers F1 dopo 15 epoche: 0.596238917097067


CON SPEAKER (BertFreezed):
emotions F1 a inizio training: 0.296031233665951
triggers F1 a inizio training: 0.545299426130769

emotions F1 dopo 6 epoche: 0.347816715874877
triggers F1 dopo 6 epoche: 0.420490221702411


CON SPEAKER (emotion: 0.65/ triggers: 0.35) (BertFreezed):
emotions F1 a inizio training: 0.263610382307591
triggers F1 a inizio training: 0.539729952038716

emotions F1 dopo 6 epoche: 0.375031700713062
triggers F1 dopo 6 epoche: 0.603563776258877

emotions F1 dopo 10 epoche: 0.418586080483474
triggers F1 dopo 10 epoche: 0.617924622326031


CON SPEAKER (emotion: 0.65/ triggers: 0.35) (BertFull -> batch_size = 2):
emotions F1 a inizio training: 0.408163126509894
triggers F1 a inizio training: 0.620237158523691

emotions F1 dopo 6 epoche: 0.563882316683445
triggers F1 dopo 6 epoche: 0.611257008535481



##############################################################################################################
##############################################################################################################
##########################################   CONSIDERAZIONI    ###############################################

Emotion e triggers overfittano a velocità diverse, usare l'early stopper non ha senso se monitoriamo la loro loss
aggregata. Non so se lo facciamo, però teniamo conto di questo aspetto se dovessi usarlo. La velocità di overfitting
delle due teste cambia anche in base alla configurazione del modello che stiamo usando (In bertFreed overfittano
entrambe più lentamente, mentre in BertFull nonostante la media pesata nel calcolo della loss la trigger head
overfitta lo stesso, forse aggiungere un dropout al FullBert aiuterebbe)

Per risolvere problema dell'overfitting a velocità diverse, opzioni:
- Usare qualche tecnica di regolarizzazione per rallentare l'apprendimento della head che classifica i triggers (dropout).
- Giocare con i pesi delle due loss
